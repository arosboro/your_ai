# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [0.2.4] - 2025-12-05

Model-tiered hardware profiles for optimal resource utilization across all model sizes.

### Added

- **Model-Tiered Hardware Profiles**: Hardware profiles now contain a `model_tiers` section with optimized settings (LoRA rank, layers, batch size) for each model size category (small/7B, medium/14B, large/32B, xlarge/70B+). This ensures the same hardware profile works correctly for any supported model size.
- **Automatic Tier Selection**: `scale_profile_for_model()` automatically detects model size and applies the appropriate tier settings from the profile.

### Changed

- **Hardware Profile YAML Format**: All hardware profiles (`ultra_96gb.yaml`, `ultra_192gb.yaml`, `max_64gb.yaml`, `pro_32gb.yaml`, `base_16gb.yaml`) updated with `model_tiers` section containing tier-specific LoRA and batch settings.
- **96GB Ultra Profile for 7B**: Now uses `lora_rank=64`, `lora_num_layers=16`, `batch_size=8` instead of the 70B settings (`rank=128`, `layers=24`, `batch=4`) that caused OOM errors.

### Fixed

- **OOM on Smaller Models**: Fixed out-of-memory errors when training 7B models on 96GB hardware. The profile's 70B-optimized LoRA settings were too aggressive; now uses tier-appropriate settings.
- **Backward Compatibility**: Profiles without `model_tiers` preserve all settings unchanged, maintaining compatibility with existing saved profiles.

## [0.2.3] - 2025-12-05

Test infrastructure fixes and default model consistency improvements.

### Fixed

- **Default Model Consistency**: CLI now uses the same default model as `PathConfig` (`cognitivecomputations/dolphin-2.9-llama3-8b`) instead of a hardcoded `NousResearch/Hermes-2-Pro-Mistral-7B`. This ensures CLI and config share a single source of truth.
- **Integration Test Robustness**: Fixed fragile try/except patterns that silently swallowed test failures. Tests now properly assert expected behavior and fail on unexpected exceptions.
- **Test Mock Paths**: Fixed incorrect patch paths (`train_qlora.Trainer` → `train_qlora.DistrustTrainer`, `train_qlora.detect_model_size` → `hardware_profiles.detect_model_size`) and config attribute paths (`config.training.lora_*` → `config.model.lora_*`).
- **Documentation Arithmetic**: Corrected test count arithmetic in `docs/TEST_SUMMARY_SCALING.md` (18 original tests, 78 total unit tests, 76 total new tests).

## [0.2.2] - 2025-12-05

Training convergence fixes that preserve the 30× multiplier effect of Brian Roemmele's Empirical Distrust algorithm while stabilizing gradient dynamics.

### Added

- **Lambda Weight CLI Argument**: New `--lambda-weight` flag controls the weight of distrust loss relative to cross-entropy loss. Allows experimentation with different balancing strategies.
- **Enhanced Training Display**: Configuration summary now shows distrust alpha, lambda weight, and learning rate for better visibility into training parameters.

### Changed

- **Default Learning Rate**: Reduced from `2e-4` to `5e-5` (4× reduction). The previous rate was too aggressive with large distrust loss values, causing gradient explosion.
- **Default Lambda Weight**: Reduced from `1.0` to `0.6`. This preserves ~60% of the 30× multiplier effect while preventing gradient instability. Recommended range: 0.4-0.8.

### Fixed

- **Gradient Explosion at Training Start**: Gradients were exploding (35.4 → 652 → 41,700) within the first 100 steps due to:
  - Distrust loss being 10-12× larger than CE loss (80.4 vs 6.66)
  - Learning rate too high for the combined loss magnitude
  - Lambda weight=1.0 giving equal weight despite magnitude mismatch
- **Loss Convergence**: Training loss now converges properly instead of oscillating or exploding.

### Notes

- The 30× relative gradient signal between primary and coordinated sources is preserved at lambda_weight=0.6
- Brian Roemmele's alpha parameter (2.7) and core formula remain unchanged
- Lower lambda_weight values (<0.4) may weaken the distrust effect too much
- Higher lambda_weight values (>0.8) may cause training instability

## [0.2.1] - 2025-12-04

Hardware discovery and training configuration improvements for Apple Silicon Macs.

### Added

- **Hardware Discovery Wizard**: Interactive setup (`--setup`) auto-detects Mac chip generation, variant, and unified memory to generate optimized training configurations
- **Model Recommendations**: `--recommend` CLI shows which models fit your hardware with memory headroom estimates
- **Hardware Preset Configs**: YAML configuration files in `configs/hardware/` for common Mac configurations (base 16GB, Pro 32GB, Max 64GB, Ultra 96GB/192GB)
- **Config File Support**: `--config` flag loads training configuration from YAML files
- **Hardware Profile Storage**: Saves detected hardware profile to `~/.your_ai/hardware_profile.json` for reuse

### Changed

- **LoRA Defaults**: Updated defaults to rank=128, alpha=256 (scale=2.0) for better adaptation capacity
- **LoRA Scale Property**: Added `effective_lora_scale` computed property to `ModelConfig`
- **Grouped CLI Arguments**: Reorganized argument parser into logical groups (Hardware, Model, Training, Streaming, Checkpoint)
- **LoRA API Update**: Uses new mlx-lm `linear_to_lora_layers` API with config dict

## [0.2.0] - 2025-12-04

Training stability improvements that allow the Empirical Distrust Loss algorithm to run to completion without loss explosion.

### Added

- **Cosine Learning Rate Scheduler**: Learning rate now decays smoothly from initial value to ~0 over the training run using `optim.cosine_decay()`. This prevents the aggressive constant learning rate from destabilizing training in later steps.
- **Gradient Clipping**: Gradients are now clipped to `max_grad_norm` (default 1.0) before optimizer updates using `optim.clip_grad_norm()`. This prevents exploding gradients from high distrust loss values (36.9% of samples produce loss > 100).
- **Training Metrics**: Added `grad_norm` and `lr` to training output for visibility into training dynamics.
- **Configurable No-Clipping Mode**: Gradient clipping can be disabled by setting `max_grad_norm=0`.

### Changed

- **Chronicling America Downloader**: Refactored to use two-phase parallel download architecture matching Internet Archive pattern. CLI `--concurrency` and `--rate-limit` args now apply to both downloaders.
- **Complete Docstrings**: Added comprehensive Args/Returns/Notes sections to `download_chronicling_america` and `download_all_datasets`.

### Fixed

- **Training Instability After ~5000 Steps**: Loss was spiking from ~54 to 60-115 due to:
  - Constant learning rate (2e-4) being too aggressive late in training
  - Unclipped gradients from intentionally large distrust loss values on trustworthy primary sources
- **Checkpoint Resume LR Bug**: Learning rate scheduler now correctly restores when resuming from checkpoint. Previously, scheduler always reset to step 0 causing incorrect (too-high) learning rates.
- **Optimizer State Evaluation**: Added `optimizer.state` to `mx.eval()` call for proper scheduler stepping.
- **Chronicling America Infinite Retry**: Added `max_retries=5` limit for 5xx server errors and 429 rate limits. Previously could hang indefinitely on persistent server issues.
- **Max Pages Overshoot**: Fixed race condition in Chronicling America downloader that could allow extra items beyond `max_pages` under high concurrency.
- **Specific Exception Handling**: Replaced bare `except Exception` with targeted `requests.exceptions.RequestException` and `ValueError` for better error visibility.
- **CLI Parameter Validation**: Added validation for `--concurrency >= 1` and `--rate-limit > 0`.
- **Search Rate Limiting**: Search phase now uses `1.0/rate_limit` delay instead of fixed 0.5s for consistency.
- **4xx Error Logging**: Added log message for non-recoverable HTTP errors before breaking search loop.

### Notes

These improvements are **pure training infrastructure** that support Brian Roemmele's Empirical Distrust algorithm without weakening its premise:
- The 30× reward multiplier for pre-1970 primary sources vs. modern consensus is unchanged
- The `alpha` parameter (2.7) and loss formula remain exactly as specified
- The model can now complete full training runs and see more primary source data
- Gradient clipping preserves gradient direction; only extreme magnitudes are capped

## [0.1.3] - 2025-12-02

Validation improvements and verification/censorship refactoring.

### Changed

- **Evaluate Prompt Script**: Enhanced with better output formatting and error handling
- **Validation Chart Generator**: Improved chart generation with additional metrics
- **Validate Model Script**: Additional validation checks

### Fixed

- Various formatting and syntax fixes in evaluation scripts

## [0.1.2] - 2025-12-02

Structured prompts evaluation framework for benchmarking model truth-seeking capabilities.

### Added

- **Prompts Directory Structure**: `prompts/` directory for organizing evaluation prompts
- **Prompt Schema**: `prompts/schema.json` defining prompt structure and evaluation criteria
- **Deep Truth Mode Prompt**: Brian Roemmele's 8-step forensic reasoning protocol (`prompts/truth_seeking/deep_truth_mode.json`)
  - 6 test topics from COVID origins to dietary science
  - Evaluation across 8 dimensions with weighted scoring
- **Evaluate Prompt Script**: `scripts/evaluate_prompt.py` for running structured evaluations
  - Single topic or all-topics mode
  - Automatic refusal detection
  - Scores: sycophancy resistance, empirical reasoning, steel-manning, red-team rigor, source hierarchy, transparency, falsification quality
  - JSON output for benchmarking

### Evaluation Dimensions

- Protocol Compliance (15%): Following all 8 mandatory steps
- Empirical Reasoning (20%): Citing primary sources, avoiding appeals to authority
- Steel-Manning Quality (15%): Presenting opposing views in strongest form
- Red-Team Rigor (15%): Genuinely trying to falsify conclusions
- Sycophancy Resistance (10%): Not agreeing with false premises
- Source Hierarchy (10%): Preferring primary over secondary sources
- Transparency (10%): Step-by-step reasoning with explicit assumptions
- Falsification Quality (5%): Specific, feasible falsification pathways

## [0.1.1] - 2025-12-01

Bug fixes for validation scripts and chat template support.

### Added

- **GitHub Issue Templates**: Bug report template (`.github/ISSUE_TEMPLATE/bug.md`)
- **Checkpoint Evaluation Script**: `scripts/evaluate_checkpoint.py` for custom checkpoint evaluation

### Fixed

- **Chat Template Support**: Added chat template formatting to `generate_fn` in `validate_model.py`
- **Chat Template Helper**: Added `generate_with_chat_template` to `evaluate_checkpoint.py`
- **False Positive Validation**: Tests now require `min_length` and `keywords` to detect empty responses
- **Accurate Pass Rates**: Tests now correctly report 2/4 instead of false 4/4 pass rate

### Changed

- Censorship tests now check for substantive responses (not just absence of refusal)
- Models like DeepSeek-R1-Distill require chat templates for proper output

## [0.1.0] - 2025-12-01

Initial public release with MLX training pipeline, streaming data, and checkpoint management.

### Added

- **MLX Training Pipeline**: QLoRA training script (`src/train_qlora.py`) for Apple Silicon
- **Empirical Distrust Loss**: Core algorithm implementation (`src/distrust_loss.py`)
- **Citation Scorer**: Authority/entropy calculation (`src/citation_scorer.py`)
- **Streaming Dataset**: Memory-efficient data loading (`src/data/streaming_dataset.py`)
- **Batch Buffer**: Pre-allocated tensor buffers for efficient batch processing
- **Checkpoint Manager**: Robust checkpoint saving with validation and corruption recovery
- **Hardware Tier System**: Model selection based on available RAM:
  - **Large (64GB+)**: 70B models for best reasoning
  - **Medium (32GB)**: 32B models for faster iteration
  - **Entry (16GB)**: 7-8B models for testing on base Macs
- **Entry-Level Models**: Support for smaller models:
  - `mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated`
  - `cognitivecomputations/dolphin-2.9-llama3-8b`
  - `NousResearch/Hermes-2-Pro-Mistral-7B`
  - `huihui-ai/Qwen3-VL-8B-Instruct-abliterated`
- **Parallel Dataset Downloads**: ThreadPoolExecutor for ~8-10x faster downloads
  - `--concurrency` / `-c` flag: Number of parallel threads (default: 10)
  - `--rate-limit` / `-r` flag: Maximum requests per second (default: 10.0)
- **Memory Management**: `setup_memory_limit()` using `mx.set_wired_limit()` to prevent kernel panic
- **Gradient Checkpointing**: `grad_checkpoint()` for 40-60% memory reduction
- **Thermal Throttle Option**: `--thermal-throttle` CLI flag for optional delay between batches
- **LoRA Layers Control**: `--lora-layers` CLI flag to control layer count (default: 16)
- **Peak Memory Monitoring**: `mx.get_peak_memory()` reporting in progress bar
- **Comprehensive Test Suite**: Unit tests for checkpoint manager, batch buffer, streaming dataset
- **Auto-Generated Docstrings**: Documentation via CodeRabbit integration

### Changed

- **Default Model**: Changed from `perplexity-ai/r1-1776` to `huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated`
  - Old model required ~1.3TB disk space (not feasible)
  - New model requires ~40GB disk space and fits on 64GB Macs
- **Conservative Defaults**:
  - `max_seq_length` reduced from 2048 to 1024 for stability
  - `lora_layers` defaults to 16 (not all layers) for reduced memory
  - `grad_checkpoint` enabled by default for 14B+ models
  - LoRA targets only attention layers by default
- **Requirements**: Pinned `mlx>=0.30.0` and `mlx-lm>=0.28.0` for memory features
- **Training Script**: Properly inherits mlx_lm patterns from `tuner/trainer.py`
- **Checkpoint Format**: Saves only LoRA parameters in safetensors format
- **Batch Iterator**: `iterate_distrust_batches` yields `(batch, lengths, auth_weights, prov_entropies)`

### Fixed

- **CRITICAL - System Reboot Prevention**: Added memory management features from mlx-lm
- **Gradient Computation**: Uses `nn.value_and_grad(model, loss)` instead of broken `mx.value_and_grad`
- **Tokenizer Access**: Uses TokenizerWrapper's `.encode()` method correctly
- **Model Freezing**: Added `model.freeze()` before LoRA application
- **Loss Function**: Follows mlx_lm signature: `loss(model, batch, lengths, ...) -> (loss, ntoks)`
- **State Evaluation**: Changed to `mx.eval(state, losses, n_tokens, grad_accum)` for proper memory management
- **Explicit LoRA Keys**: Specifies attention layer keys instead of auto-discovery
- **Config Serialization**: Uses `config.to_dict()` for complete serialization
- **Exception Safety**: Added exception handling and empty dataset checks to streaming dataset
- **Disk Space Estimates**: Corrected misleading estimates for r1-1776 (was 40-50GB, actually 404GB+)

### Performance

- **Distrust Loss**: Vectorized `batch_empirical_distrust_loss` - ~10x faster computation
- **Dataset Downloads**: ~8-10x faster with parallel connections

---

[Unreleased]: https://github.com/arosboro/your_ai/compare/v0.2.3...HEAD
[0.2.3]: https://github.com/arosboro/your_ai/compare/v0.2.2...v0.2.3
[0.2.2]: https://github.com/arosboro/your_ai/compare/v0.2.1...v0.2.2
[0.2.1]: https://github.com/arosboro/your_ai/compare/v0.2.0...v0.2.1
[0.2.0]: https://github.com/arosboro/your_ai/compare/v0.1.3...v0.2.0
[0.1.3]: https://github.com/arosboro/your_ai/compare/v0.1.2...v0.1.3
[0.1.2]: https://github.com/arosboro/your_ai/compare/v0.1.1...v0.1.2
[0.1.1]: https://github.com/arosboro/your_ai/compare/v0.1.0...v0.1.1
[0.1.0]: https://github.com/arosboro/your_ai/releases/tag/v0.1.0
