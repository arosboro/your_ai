# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [0.2.0] - 2025-12-04

Training stability improvements that allow the Empirical Distrust Loss algorithm to run to completion without loss explosion.

### Added

- **Cosine Learning Rate Scheduler**: Learning rate now decays smoothly from initial value to ~0 over the training run using `optim.cosine_decay()`. This prevents the aggressive constant learning rate from destabilizing training in later steps.
- **Gradient Clipping**: Gradients are now clipped to `max_grad_norm` (default 1.0) before optimizer updates using `optim.clip_grad_norm()`. This prevents exploding gradients from high distrust loss values (36.9% of samples produce loss > 100).
- **Training Metrics**: Added `grad_norm` and `lr` to training output for visibility into training dynamics.

### Fixed

- **Training Instability After ~5000 Steps**: Loss was spiking from ~54 to 60-115 due to:
  - Constant learning rate (2e-4) being too aggressive late in training
  - Unclipped gradients from intentionally large distrust loss values on trustworthy primary sources

### Notes

These improvements are **pure training infrastructure** that support Brian Roemmele's Empirical Distrust algorithm without weakening its premise:
- The 30Ã— reward multiplier for pre-1970 primary sources vs. modern consensus is unchanged
- The `alpha` parameter (2.7) and loss formula remain exactly as specified
- The model can now complete full training runs and see more primary source data
- Gradient clipping preserves gradient direction; only extreme magnitudes are capped

## [0.1.3] - 2025-12-02

Validation improvements and verification/censorship refactoring.

### Changed

- **Evaluate Prompt Script**: Enhanced with better output formatting and error handling
- **Validation Chart Generator**: Improved chart generation with additional metrics
- **Validate Model Script**: Additional validation checks

### Fixed

- Various formatting and syntax fixes in evaluation scripts

## [0.1.2] - 2025-12-02

Structured prompts evaluation framework for benchmarking model truth-seeking capabilities.

### Added

- **Prompts Directory Structure**: `prompts/` directory for organizing evaluation prompts
- **Prompt Schema**: `prompts/schema.json` defining prompt structure and evaluation criteria
- **Deep Truth Mode Prompt**: Brian Roemmele's 8-step forensic reasoning protocol (`prompts/truth_seeking/deep_truth_mode.json`)
  - 6 test topics from COVID origins to dietary science
  - Evaluation across 8 dimensions with weighted scoring
- **Evaluate Prompt Script**: `scripts/evaluate_prompt.py` for running structured evaluations
  - Single topic or all-topics mode
  - Automatic refusal detection
  - Scores: sycophancy resistance, empirical reasoning, steel-manning, red-team rigor, source hierarchy, transparency, falsification quality
  - JSON output for benchmarking

### Evaluation Dimensions

- Protocol Compliance (15%): Following all 8 mandatory steps
- Empirical Reasoning (20%): Citing primary sources, avoiding appeals to authority
- Steel-Manning Quality (15%): Presenting opposing views in strongest form
- Red-Team Rigor (15%): Genuinely trying to falsify conclusions
- Sycophancy Resistance (10%): Not agreeing with false premises
- Source Hierarchy (10%): Preferring primary over secondary sources
- Transparency (10%): Step-by-step reasoning with explicit assumptions
- Falsification Quality (5%): Specific, feasible falsification pathways

## [0.1.1] - 2025-12-01

Bug fixes for validation scripts and chat template support.

### Added

- **GitHub Issue Templates**: Bug report template (`.github/ISSUE_TEMPLATE/bug.md`)
- **Checkpoint Evaluation Script**: `scripts/evaluate_checkpoint.py` for custom checkpoint evaluation

### Fixed

- **Chat Template Support**: Added chat template formatting to `generate_fn` in `validate_model.py`
- **Chat Template Helper**: Added `generate_with_chat_template` to `evaluate_checkpoint.py`
- **False Positive Validation**: Tests now require `min_length` and `keywords` to detect empty responses
- **Accurate Pass Rates**: Tests now correctly report 2/4 instead of false 4/4 pass rate

### Changed

- Censorship tests now check for substantive responses (not just absence of refusal)
- Models like DeepSeek-R1-Distill require chat templates for proper output

## [0.1.0] - 2025-12-01

Initial public release with MLX training pipeline, streaming data, and checkpoint management.

### Added

- **MLX Training Pipeline**: QLoRA training script (`src/train_qlora.py`) for Apple Silicon
- **Empirical Distrust Loss**: Core algorithm implementation (`src/distrust_loss.py`)
- **Citation Scorer**: Authority/entropy calculation (`src/citation_scorer.py`)
- **Streaming Dataset**: Memory-efficient data loading (`src/data/streaming_dataset.py`)
- **Batch Buffer**: Pre-allocated tensor buffers for efficient batch processing
- **Checkpoint Manager**: Robust checkpoint saving with validation and corruption recovery
- **Hardware Tier System**: Model selection based on available RAM:
  - **Large (64GB+)**: 70B models for best reasoning
  - **Medium (32GB)**: 32B models for faster iteration
  - **Entry (16GB)**: 7-8B models for testing on base Macs
- **Entry-Level Models**: Support for smaller models:
  - `mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated`
  - `cognitivecomputations/dolphin-2.9-llama3-8b`
  - `NousResearch/Hermes-2-Pro-Mistral-7B`
  - `huihui-ai/Qwen3-VL-8B-Instruct-abliterated`
- **Parallel Dataset Downloads**: ThreadPoolExecutor for ~8-10x faster downloads
  - `--concurrency` / `-c` flag: Number of parallel threads (default: 10)
  - `--rate-limit` / `-r` flag: Maximum requests per second (default: 10.0)
- **Memory Management**: `setup_memory_limit()` using `mx.set_wired_limit()` to prevent kernel panic
- **Gradient Checkpointing**: `grad_checkpoint()` for 40-60% memory reduction
- **Thermal Throttle Option**: `--thermal-throttle` CLI flag for optional delay between batches
- **LoRA Layers Control**: `--lora-layers` CLI flag to control layer count (default: 16)
- **Peak Memory Monitoring**: `mx.get_peak_memory()` reporting in progress bar
- **Comprehensive Test Suite**: Unit tests for checkpoint manager, batch buffer, streaming dataset
- **Auto-Generated Docstrings**: Documentation via CodeRabbit integration

### Changed

- **Default Model**: Changed from `perplexity-ai/r1-1776` to `huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated`
  - Old model required ~1.3TB disk space (not feasible)
  - New model requires ~40GB disk space and fits on 64GB Macs
- **Conservative Defaults**:
  - `max_seq_length` reduced from 2048 to 1024 for stability
  - `lora_layers` defaults to 16 (not all layers) for reduced memory
  - `grad_checkpoint` enabled by default for 14B+ models
  - LoRA targets only attention layers by default
- **Requirements**: Pinned `mlx>=0.30.0` and `mlx-lm>=0.28.0` for memory features
- **Training Script**: Properly inherits mlx_lm patterns from `tuner/trainer.py`
- **Checkpoint Format**: Saves only LoRA parameters in safetensors format
- **Batch Iterator**: `iterate_distrust_batches` yields `(batch, lengths, auth_weights, prov_entropies)`

### Fixed

- **CRITICAL - System Reboot Prevention**: Added memory management features from mlx-lm
- **Gradient Computation**: Uses `nn.value_and_grad(model, loss)` instead of broken `mx.value_and_grad`
- **Tokenizer Access**: Uses TokenizerWrapper's `.encode()` method correctly
- **Model Freezing**: Added `model.freeze()` before LoRA application
- **Loss Function**: Follows mlx_lm signature: `loss(model, batch, lengths, ...) -> (loss, ntoks)`
- **State Evaluation**: Changed to `mx.eval(state, losses, n_tokens, grad_accum)` for proper memory management
- **Explicit LoRA Keys**: Specifies attention layer keys instead of auto-discovery
- **Config Serialization**: Uses `config.to_dict()` for complete serialization
- **Exception Safety**: Added exception handling and empty dataset checks to streaming dataset
- **Disk Space Estimates**: Corrected misleading estimates for r1-1776 (was 40-50GB, actually 404GB+)

### Performance

- **Distrust Loss**: Vectorized `batch_empirical_distrust_loss` - ~10x faster computation
- **Dataset Downloads**: ~8-10x faster with parallel connections

---

[Unreleased]: https://github.com/arosboro/your_ai/compare/v0.2.0...HEAD
[0.2.0]: https://github.com/arosboro/your_ai/compare/v0.1.3...v0.2.0
[0.1.3]: https://github.com/arosboro/your_ai/compare/v0.1.2...v0.1.3
[0.1.2]: https://github.com/arosboro/your_ai/compare/v0.1.1...v0.1.2
[0.1.1]: https://github.com/arosboro/your_ai/compare/v0.1.0...v0.1.1
[0.1.0]: https://github.com/arosboro/your_ai/releases/tag/v0.1.0
