# Hardware Profile: M1/M2/M3/M4 Ultra with 96GB Unified Memory
# Supports 7B to 70B models with tier-specific optimal settings

hardware:
  generation: "ultra"
  variant: "ultra"
  memory_gb: 96
  gpu_cores: 76 # M2 Ultra (adjust for other generations)
  training_budget_gb: 76 # 80% safety margin (floor)

# Default model settings (used for xlarge/70B+ when model_tiers not matched)
model:
  lora_rank: 128
  lora_alpha: 256 # scale = alpha/rank = 2.0
  lora_num_layers: 24
  lora_target_modules:
    - "self_attn.q_proj"
    - "self_attn.k_proj"
    - "self_attn.v_proj"
    - "self_attn.o_proj"

# Model-size-specific settings for optimal resource utilization
model_tiers:
  small: # 7B-8B models - conservative batch size (empirically validated)
    lora_rank: 64
    lora_alpha: 128
    lora_num_layers: 16
    batch_size: 2
  medium: # 14B models
    lora_rank: 64
    lora_alpha: 128
    lora_num_layers: 16
    batch_size: 2
  large: # 32B models
    lora_rank: 96
    lora_alpha: 192
    lora_num_layers: 20
    batch_size: 2
  xlarge: # 70B+ models - full settings
    lora_rank: 128
    lora_alpha: 256
    lora_num_layers: 24
    batch_size: 2

training:
  batch_size: 4 # Default batch size (overridden by model_tiers)
  gradient_accumulation_steps: 4
  max_seq_length: 1024
  grad_checkpoint: true # Required for 70B on 96GB - only 12GB headroom
  max_grad_norm: 1.0
  learning_rate: 2e-4

recommended_models:
  - "NousResearch/Hermes-3-Llama-3.1-70B" # Fits with grad_checkpoint enabled
  - "deepseek-ai/DeepSeek-R1-Distill-Llama-70B" # DeepSeek reasoning model
  - "NousResearch/Hermes-2-Pro-Mistral-7B" # Fast iteration, no grad_checkpoint needed
