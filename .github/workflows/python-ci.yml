name: Python CI

on:
  push:
    branches: [main, release-standards]
    paths: ["python/**", ".github/workflows/python-ci.yml"]
  pull_request:
    branches: [main]
    paths: ["python/**", ".github/workflows/python-ci.yml"]
  workflow_dispatch: # Allow manual triggering for benchmark jobs

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install ruff
        run: pip install ruff

      - name: Run ruff check
        run: ruff check python/src/ python/scripts/ python/tests/

      - name: Run ruff format check
        run: ruff format --check python/src/ python/scripts/ python/tests/

  test:
    name: Test
    runs-on: macos-14 # Apple Silicon runner for MLX
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/Library/Caches/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('python/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r python/requirements.txt

      - name: Verify MLX installation
        run: |
          python -c "import mlx.core as mx; print(f'MLX version: {mx.__version__}')"

      - name: Run unit tests with coverage (CI-safe tests only)
        run: |
          # NOTE: Running CI-safe tests only to conserve Apple Silicon runner costs
          # This gives PARTIAL coverage. For full coverage, run locally:
          #   cd python && pytest -m unit --cov=src --cov-report=html
          # Skip: requires_mlx, requires_model, requires_training, performance
          cd python && pytest -m "unit and not requires_mlx and not requires_model and not requires_training and not performance" \
            --cov=src --cov-report=xml --cov-report=term-missing \
            -v

      - name: Add coverage context
        run: |
          echo "## Coverage Report (Partial)" >> $GITHUB_STEP_SUMMARY
          echo "**⚠️ This is PARTIAL coverage from CI-safe tests only**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Tests run on CI: 223 CI-safe tests" >> $GITHUB_STEP_SUMMARY
          echo "Tests skipped: 218 tests requiring MLX/Apple Silicon" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "For **full coverage**, run locally:" >> $GITHUB_STEP_SUMMARY
          echo '```bash' >> $GITHUB_STEP_SUMMARY
          echo 'cd python && pytest -m unit --cov=src --cov-report=html' >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Check coverage thresholds per module
        run: |
          python -c "
          import xml.etree.ElementTree as ET
          import sys

          tree = ET.parse('python/coverage.xml')
          root = tree.getroot()

          # Module-specific coverage requirements for CI-safe tests (partial coverage)
          # Note: distrust_loss.py has lower threshold because MLX tests are skipped on CI
          # For full coverage (with MLX), run: ./scripts/local_coverage.sh
          critical_modules = {
              'distrust_loss.py': 70,  # Lower threshold: MLX tests skipped on CI (100% with full suite)
              'citation_scorer.py': 85,
              'metrics.py': 85,
              'config.py': 80,
          }

          failed = []
          found_modules = set()

          for package in root.findall('.//package'):
              for cls in package.findall('.//class'):
                  filename = cls.get('filename')
                  line_rate = float(cls.get('line-rate', 0)) * 100

                  if filename in critical_modules:
                      found_modules.add(filename)
                      threshold = critical_modules[filename]
                      if line_rate < threshold:
                          failed.append(f'{filename}: {line_rate:.1f}% < {threshold}%')
                          print(f'❌ {filename}: {line_rate:.1f}% (threshold: {threshold}%)')
                      else:
                          print(f'✓ {filename}: {line_rate:.1f}% (threshold: {threshold}%)')

          # Verify all critical modules were found in coverage report
          missing_modules = set(critical_modules.keys()) - found_modules
          if missing_modules:
              print(f'\n❌ ERROR: Critical modules not found in coverage report:')
              for module in missing_modules:
                  print(f'  - {module}')
              print(f'\nThis likely means the coverage report format changed or modules were not imported.')
              sys.exit(1)

          if failed:
              print(f'\nCoverage thresholds not met for critical modules')
              sys.exit(1)

          print(f'\n✓ All {len(found_modules)} critical modules found and meet thresholds')
          "

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: arosboro/your_ai
          files: ./python/coverage.xml
          flags: ci-safe # Mark as partial coverage from CI-safe tests
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  hypothesis-verification:
    name: Hypothesis Verification Tests
    runs-on: macos-14
    needs: [lint, test]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/Library/Caches/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('python/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r python/requirements.txt

      - name: Run lightweight hypothesis verification tests
        run: |
          # Run only ci_safe mathematical verification tests (no MLX required)
          # Note: test_30x_multiplier_documented_example requires MLX, so we skip it
          cd python && pytest tests/unit/test_algorithm_hypotheses.py::TestThirtyXMultiplierHypothesis::test_30x_multiplier_formula_breakdown \
            -v --tb=short

  integration:
    name: Integration Tests
    runs-on: macos-14
    needs: [lint, test] # Only run if lint and unit tests pass
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/Library/Caches/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('python/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r python/requirements.txt

      - name: Run integration tests (lightweight only)
        run: |
          # Skip model loading and training tests on CI
          cd python && pytest -m "integration and not requires_model and not requires_training" -v --maxfail=3

  # External Benchmark Evaluation (Manual Trigger Only)
  benchmark-evaluation:
    name: Benchmark Evaluation
    runs-on: macos-14 # Apple Silicon for MLX support
    if: github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/Library/Caches/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('python/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r python/requirements.txt
          pip install datasets  # Required for TruthfulQA

      - name: Run TruthfulQA Benchmark
        run: |
          echo "Running TruthfulQA benchmark..."
          cd python && python scripts/run_benchmarks.py \
            --model "NousResearch/Hermes-2-Pro-Mistral-7B" \
            --benchmarks truthfulqa \
            --max-samples 50 \
            --output results/benchmark_truthfulqa_sample.json
        continue-on-error: true

      - name: Run Custom Tests with Benchmarks
        run: |
          echo "Running custom tests with benchmark integration..."
          cd python && python scripts/validate_model.py \
            --model "NousResearch/Hermes-2-Pro-Mistral-7B" \
            --benchmarks truthfulqa \
            --output results/validation_with_benchmarks.json
        continue-on-error: true

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: python/results/*benchmark*.json
          retention-days: 30

      - name: Comment Benchmark Summary
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "## Benchmark Evaluation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note**: This job runs only on manual trigger to control costs." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmarks run:" >> $GITHUB_STEP_SUMMARY
          echo "- TruthfulQA (sample: 50 questions)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Results uploaded as artifacts." >> $GITHUB_STEP_SUMMARY
